<!doctype html>
<html>
    <head>
        <meta charset="utf-8" />
        <meta http-equiv="X-UA-Compatible" content="chrome=1" />
        <title>Regularisation & Redundancy: Examining Generalisation in Deep Learning</title>

        <link rel="icon" type="image/svg+xml" sizes="any" type="/favicon.svg" />
        <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png" />
        <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png" />

        <style>
            @import url("https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;500&family=Fira+Sans:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&family=Merriweather:ital,wght@0,300;0,400;1,300;1,400&display=swap");
        </style>
        <link rel="stylesheet" href="../../css/main.min.css?v=abe9c2" />

        
        <!-- KaTeX -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous" />
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
        <!-- End KaTeX Code -->
        

        <meta name="google-site-verification" content="zktyx9NWxN4_455e9TF-WP6Fh3AYo0Hs62TYwr43QIY" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
    </head>
    <body>
        <header>June 19, 2023 / <a href="../../" rel="author">Ng Wei En</a> / #ml-theory</header>
<h1>Regularisation & Redundancy: Examining Generalisation in Deep Learning</h1>

<div id="content">
<nav id="toc" role="doc-toc">
<strong>Contents</strong>
<label for="contents">⊕</label>
<input type="checkbox" id="contents">
<ul>
<li><a href="#simple-models" id="toc-simple-models">Simple Models?</a></li>
<li><a href="#regularisation" id="toc-regularisation">Regularisation</a>
<ul>
<li><a href="#bias-variance-tradeoff" id="toc-bias-variance-tradeoff">Bias-Variance Tradeoff</a></li>
<li><a href="#double-descent-phenomenon" id="toc-double-descent-phenomenon">Double-Descent Phenomenon</a></li>
</ul></li>
<li><a href="#redundancy" id="toc-redundancy">Redundancy</a>
<ul>
<li><a href="#network-sparsity" id="toc-network-sparsity">Network Sparsity</a></li>
<li><a href="#quantisation" id="toc-quantisation">Quantisation</a></li>
</ul></li>
<li><a href="#implications" id="toc-implications">Implications</a></li>
</ul>
</nav>
<p>The past decade has seen deep learning models grow massively in size, as
researchers gain access to greater levels of processing power. These models
enter the regime of <a href="https://arxiv.org/abs/2109.02355" title="A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning">overparameterisation</a>, where the number of learnable
parameters far exceed the number of training examples<span class="sidenote-wrapper"><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle" /><span class="sidenote">The number of model parameters may not be a good proxy for model complexity
in deep learning, and what constitutes a good definition of model complexity
remains an open question. See <a href="https://arxiv.org/abs/2109.02355" title="A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning">Dar et al. (2021)</a>.<br />
<br />
</span></span>, and yet achieve
surprisingly high performance on unseen data in complex tasks.</p>
<p>Rather than explain existing theories of generalisation, which at present are
still very diverse and complex, this article seeks to examine some recent
fascinating empirical findings in <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional Neural Networks (CNNs)</a>
from two different perspectives:</p>
<ol type="1">
<li>Performing <strong>regularisation</strong>, which encourages “simpler” or more
“structured” models in order to improve generalisation; and</li>
<li>Reducing <strong>redundancy</strong>, which compresses models to reduce memory
requirements.</li>
</ol>
<p>These two perspectives revolve around a central question in the design of deep
learning architectures: <strong>what makes massive neural network architectures so
effective</strong>, and <strong>can smaller or simpler architectures do as well?</strong></p>
<h2 id="simple-models">Simple Models?</h2>
<p>[TODO: add FNN diagram]</p>
<p>The <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" title="Universal approximation theorem">universal approximation theorem</a> is often cited as the underlying
reason behind the expressivity of neural networks - essentially, under <em>certain
conditions</em>, a <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network" title="Feedforward neural network">Feedforward Network (FNN)</a> can approximate any continuous
function:</p>
<ol type="1">
<li><strong>(Arbitrary width)</strong> <a href="https://dl.acm.org/doi/abs/10.5555/70405.70408" title="Multilayer feedforward networks are universal approximators">Kurt et al. (1989)</a> proved for a FNN with <em>a
single hidden layer of arbitrary width</em> and the <a href="https://en.wikipedia.org/wiki/Hyperbolic_functions" title="Hyperbolic functions">tanh</a> activation
function.</li>
<li><strong>(Arbitrary depth)</strong> <a href="https://proceedings.neurips.cc/paper/2017/hash/32cbf687880eb1674a07bf717761dd3a-Abstract.html" title="The Expressive Power of Neural Networks: A View from the Width">Lu et al. (2017)</a> proved for a FNN with
<em>arbitrarily many <span class="math inline">\((n+4)\)</span>-width hidden layers</em> and the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">ReLU</a>
activation function.</li>
</ol>
<h2 id="regularisation">Regularisation</h2>
<p>Regularisation is often seen as imposing <a href="https://en.wikipedia.org/wiki/Occam%27s_razor" title="Occam's razor">Occam’s razor</a>: between two
different models which are similarly feasible, we should prefer the “simpler”
model. Simplicity can be viewed in various ways:</p>
<ol type="1">
<li>a model with lower “flexibility” may better represent the test population,
such as in the classic example of <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html" title="Underfitting vs. Overfitting">polynomial regression</a>;</li>
<li>models <a href="https://en.wikipedia.org/wiki/Minimum_description_length" title="Minimum description length">that can be “compressed” more</a> could be considered simpler;</li>
<li>for models with many parameters, <a href="https://en.wikipedia.org/wiki/Ridge_regression" title="Ridge regression">a penalty term is often added to the loss
function</a> to achieve higher sparsity (i.e. having more parameters
close to zero);</li>
<li>certain restrictions could be imposed on the model structure, such as <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" title="Convolutional neural network">the
use of Convolutional Neural Network (CNN) architctures</a>.</li>
</ol>
<p>Intuitively, simpler models work well in simpler tasks, especially if we know
how the solution should look like; for e.g., learning a polynomial function is
straightforward even if the degree of the polynomial is unknown. This is not the
case with complex tasks such as image classification, where it is unclear why
one should prefer “simpler” model architectures over more sophisticated or
“flexible” ones. <strong>Understanding regularisation in complex tasks requires us to
revisit <em>the bias-variance tradeoff</em></strong>.</p>
<h3 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h3>
<p>Often used to justify regularisation in introductory courses, [TODO]</p>
<h3 id="double-descent-phenomenon">Double-Descent Phenomenon</h3>
<p>The effectiveness of CNNs on image tasks is often attributed to its resemblance
with the <a href="https://en.wikipedia.org/wiki/Visual_cortex" title="Visual cortex">visual cortex</a>.</p>
<figure>
<img src="../../public/ml-theory/regularisation-redundancy/cnn-arch.png" alt="CNN Architecture (Credits: Wikimedia)" />
<figcaption aria-hidden="true">CNN Architecture (Credits: <a href="https://commons.wikimedia.org/wiki/File:Typical_cnn.png">Wikimedia</a>)</figcaption>
</figure>
<h2 id="redundancy">Redundancy</h2>
<h3 id="network-sparsity">Network Sparsity</h3>
<h3 id="quantisation">Quantisation</h3>
<h2 id="implications">Implications</h2></div>

<hr />

<aside>
    <small>
        <p>
            <em>
                You can view the history of changes made to this post at
                <a href="https://github.com/wei2912/wei2912.github.io/blame/master/posts/ml-theory/regularisation-redundancy.md">this website's GitHub repository</a>.
            </em>
        </p>
    </small>
</aside>

<script src="https://utteranc.es/client.js" repo="wei2912/wei2912.github.io" issue-term="title" label="comments" theme="github-light" crossorigin="anonymous" async></script>

<hr />

        <footer>
            <small>
                <p>
                    <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">
                        <img alt="Creative Commons License" style="border-width: 0" src="https://licensebuttons.net/l/by/4.0/88x31.png" /> </a><br />
                    This work is licensed under a
                    <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International
                        License</a>.<br />
                    The source of the website is available at
                    <a href="https://github.com/wei2912/wei2912.github.io">wei2912/wei2912.github.io</a>.<br />
                </p>
            </small>
        </footer>
    </body>
</html>
